import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
import jieba

# ================= é…ç½®åŒº =================
FILE_PATH = '/Users/iris/Desktop/codingæ±‡æ€»/gemini 2.0 flash_FIAC_combined_data.xlsx'
OUTPUT_FILENAME = 'Confusion_Matrix_FIAC_gemini.pdf'

COL_MAP = {
    'Text': ['è¯¾å ‚å¯¹è¯'],
    'True_Label': ['FIAC-1', 'True_Label'],
    'Pred_Label': ['FIAC_AI_Result_j1', 'FIAC_AI_Result_j2', 'FIAC_AI_Result_j3', 'FIAC_CoT_Result',
                   'FIAC_AI_j1', 'FIAC_AI_j2', 'FIAC_AI_j3',
                   'FIAC_j1', 'FIAC_j2', 'FIAC_j3', 'Pred_Label']
}
# ==========================================

plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'PingFang HK', 'Heiti TC']
plt.rcParams['axes.unicode_minus'] = False


def map_to_fiac_cluster(label):
    """å•ç‹¬å®šä¹‰çš„æ˜ å°„å‡½æ•°ï¼Œä¸è¦åœ¨é‡Œé¢å¼•ç”¨ df"""
    label_str = str(label).strip().split('.')[0]
    import re
    match = re.search(r'\d+', label_str)
    if not match:
        return 'Unknown'
    num = match.group()
    mapping = {
        '1': '1-AcceptFeelings', '2': '2-Praise', '3': '3-AcceptIdeas',
        '4': '4-AskQuestions', '5': '5-Lecturing', '6': '6-GiveDirections',
        '7': '7-Criticize', '8': '8-StudentResponse', '9': '9-StudentInitiation',
        '10': '10-Silence'
    }
    return mapping.get(num, 'Unknown')


def analyze_fiac():
    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨ FIAC åˆ†ææµ: {FILE_PATH} ...")
    df = pd.read_excel(FILE_PATH) if FILE_PATH.endswith('.xlsx') else pd.read_csv(FILE_PATH)

    rename_dict = {}
    for target, candidates in COL_MAP.items():
        if target in df.columns: continue
        for c in candidates:
            if c in df.columns:
                rename_dict[c] = target
                break
    df = df.rename(columns=rename_dict).dropna(subset=['True_Label', 'Pred_Label'])

    df['True_Label'] = df['True_Label'].astype(str).str.strip()
    df['Pred_Label'] = df['Pred_Label'].astype(str).str.strip()

    print("ğŸ›  æ­£åœ¨åº”ç”¨ FIAC æ ‡å‡†åˆ†ç±»è§„åˆ™...")
    # æ‰§è¡Œè½¬æ¢
    df['True_Coarse'] = df['True_Label'].apply(map_to_fiac_cluster)
    df['Pred_Coarse'] = df['Pred_Label'].apply(map_to_fiac_cluster)

    # --- è¯Šæ–­ä»£ç æ”¾åœ¨è¿™é‡Œï¼ˆè½¬æ¢å®Œæˆåï¼‰ ---
    print("ğŸ“‹ [æ•°æ®è¯Šæ–­] åŸå§‹æ ‡ç­¾å‰5è¡Œ:", df['True_Label'].head().tolist())
    print("ğŸ“‹ [æ•°æ®è¯Šæ–­] è½¬æ¢åæ ‡ç­¾å‰5è¡Œ:", df['True_Coarse'].head().tolist())

    labels = [
        '1-AcceptFeelings', '2-Praise', '3-AcceptIdeas', '4-AskQuestions',
        '5-Lecturing', '6-GiveDirections', '7-Criticize',
        '8-StudentResponse', '9-StudentInitiation', '10-Silence'
    ]

    # è¿‡æ»¤æ‰ Unknown æ•°æ®
    df_clean = df[df['True_Coarse'].isin(labels) & df['Pred_Coarse'].isin(labels)].copy()

    # 2. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    if len(df_clean) == 0:
        print("âŒ é”™è¯¯ï¼šè¿‡æ»¤åæ ·æœ¬é‡ä¸º 0ï¼Œè¯·æ£€æŸ¥ Excel ä¸­çš„åŸå§‹æ ‡ç­¾æ˜¯å¦åŒ…å« 1-10 çš„æ•°å­—ã€‚")
        return

    print(f"ğŸ¨ æ­£åœ¨ç»˜åˆ¶çŸ©é˜µï¼Œæ ·æœ¬é‡: {len(df_clean)}")
    y_true, y_pred = df_clean['True_Coarse'], df_clean['Pred_Coarse']
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_norm = np.nan_to_num(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])

    plt.figure(figsize=(12, 10))
    # cmap='Blues' å®ç°äº†è“ç™½é…è‰²
    # linewidths=.5 å¢åŠ äº†æ ¼å­é—´çš„ç»†å¾®ç™½çº¿ï¼Œæ›´æ¸…çˆ½
    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=labels, yticklabels=labels,
                linewidths=.5, cbar_kws={'label': 'Proportion'})

    plt.title('Confusion Matrix: Gemini-2.0-flash (FIAC Standard)', fontsize=15, pad=20)
    plt.xlabel('Predicted FIAC Category', fontsize=12, labelpad=10)
    plt.ylabel('True FIAC Category', fontsize=12, labelpad=10)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()

    plt.savefig(OUTPUT_FILENAME, dpi=300)
    print(f"ğŸ‰ FIAC æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {OUTPUT_FILENAME}")

    # 3. å…³é”®è¯åˆ†æ
    print("\nğŸ” æ­£åœ¨åˆ†æç‰¹å¾è¯...")
    np.fill_diagonal(cm_norm, 0)
    flattened = cm_norm.flatten()
    indices = np.argsort(flattened)[::-1]
    STOP_WORDS = set(
        ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'æˆ‘ä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å—¯', 'å•Š', 'å—', 'å§', 'è€å¸ˆ', 'åŒå­¦'])

    for i in range(3):
        idx = indices[i]
        row, col = idx // len(labels), idx % len(labels)
        if cm_norm[row, col] < 0.05: continue

        true_l, pred_l = labels[row], labels[col]
        print(f"\nğŸ”´ [è¯¯åˆ¤åˆ†æ] çœŸ:ã€{true_l}ã€‘ -> è¯¯åˆ¤ä¸º:ã€{pred_l}ã€‘ (å æ¯”: {cm_norm[row, col]:.1%})")

        err_txt = df_clean[(df_clean['True_Coarse'] == true_l) & (df_clean['Pred_Coarse'] == pred_l)]['Text']
        corr_txt = df_clean[(df_clean['True_Coarse'] == true_l) & (df_clean['Pred_Coarse'] == true_l)]['Text']

        if len(err_txt) < 3: continue

        def get_words(ts):
            return [" ".join([w for w in jieba.cut(str(t)) if w not in STOP_WORDS and len(w) > 1]) for t in ts]

        try:
            vectorizer = CountVectorizer(max_features=300)
            X = vectorizer.fit_transform(get_words(err_txt) + get_words(corr_txt))
            names = vectorizer.get_feature_names_out()
            f_err = np.array(X[:len(err_txt)].sum(axis=0)).flatten() / len(err_txt)
            f_corr = np.array(X[len(err_txt):].sum(axis=0)).flatten() / len(corr_txt) + 1e-6
            ratios = f_err / f_corr
            for widx in np.argsort(ratios)[::-1][:5]:
                if ratios[widx] > 1.5: print(f"   è¯±é¥µè¯: {names[widx]:<10} | é£é™©å€æ•°: {ratios[widx]:.1f}x")
        except:
            continue


if __name__ == '__main__':
    analyze_fiac()